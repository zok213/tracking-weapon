#!/usr/bin/env python3
"""
RGBT Knowledge Distillation Training Script
Distills YOLOv26x-RGBT teacher into smaller student models.

Strategy:
- Feature-based distillation (P3, P4, P5 feature maps)
- Logit-based distillation (KL divergence with temperature)
- Standard detection loss

Usage:
    python distill_rgbt.py --teacher 26x --student 26m --epochs 30
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from datetime import datetime

# Add pipeline scripts to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'tracking/stage1/scripts'))

from ultralytics import YOLO
from ultralytics.engine.trainer import BaseTrainer
from ultralytics.models.yolo.detect import DetectionTrainer
from ultralytics.utils import LOGGER

ROOT = Path(__file__).parent.parent


class DistillationLoss(nn.Module):
    """Combined distillation loss for YOLO models."""
    
    def __init__(self, temperature=4.0, alpha=1.0, beta=0.5, gamma=0.3):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha  # Detection loss weight
        self.beta = beta    # Logit KD weight
        self.gamma = gamma  # Feature KD weight
        
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
        self.mse_loss = nn.MSELoss()
    
    def logit_distillation(self, student_logits, teacher_logits):
        """KL divergence between student and teacher logits."""
        T = self.temperature
        student_soft = F.log_softmax(student_logits / T, dim=-1)
        teacher_soft = F.softmax(teacher_logits / T, dim=-1)
        return self.kl_loss(student_soft, teacher_soft) * (T ** 2)
    
    def feature_distillation(self, student_feats, teacher_feats):
        """MSE loss between student and teacher feature maps."""
        total_loss = 0
        for s_feat, t_feat in zip(student_feats, teacher_feats):
            # Adapt dimensions if needed
            if s_feat.shape != t_feat.shape:
                # Use adaptive pooling to match spatial dimensions
                s_feat = F.adaptive_avg_pool2d(s_feat, t_feat.shape[-2:])
                # Use 1x1 conv to match channel dimensions (lazy)
                if s_feat.shape[1] != t_feat.shape[1]:
                    s_feat = F.interpolate(
                        s_feat.unsqueeze(0), 
                        size=(t_feat.shape[1], t_feat.shape[2], t_feat.shape[3]),
                        mode='nearest'
                    ).squeeze(0) if len(s_feat.shape) == 3 else s_feat
            total_loss += self.mse_loss(s_feat, t_feat.detach())
        return total_loss / len(student_feats) if student_feats else 0


class RGBTDistillationTrainer(DetectionTrainer):
    """Custom trainer for knowledge distillation with RGBT models."""
    
    def __init__(self, teacher_path, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher_path = teacher_path
        self.teacher = None
        self.distill_loss = DistillationLoss()
        
    def setup_model(self):
        """Load teacher and student models."""
        super().setup_model()
        
        # Load teacher model (frozen)
        LOGGER.info(f"üî• Loading teacher model: {self.teacher_path}")
        self.teacher = YOLO(self.teacher_path).model
        self.teacher.eval()
        self.teacher.requires_grad_(False)
        self.teacher.to(self.device)
        LOGGER.info("‚úÖ Teacher model loaded and frozen")
    
    def get_model(self, cfg=None, weights=None, verbose=True):
        """Override to ensure 4-channel student model."""
        model = super().get_model(cfg, weights, verbose)
        
        # Verify 4-channel input
        first_conv = model.model[0].conv
        if first_conv.in_channels != 4:
            LOGGER.warning(f"‚ö†Ô∏è Student has {first_conv.in_channels} channels, patching to 4...")
            new_conv = nn.Conv2d(
                4, first_conv.out_channels,
                first_conv.kernel_size, first_conv.stride,
                first_conv.padding, bias=first_conv.bias is not None
            )
            # Initialize: copy RGB weights, zero for thermal
            with torch.no_grad():
                new_conv.weight[:, :3, :, :] = first_conv.weight[:, :3, :, :]
                new_conv.weight[:, 3:, :, :] = 0
                if first_conv.bias is not None:
                    new_conv.bias = first_conv.bias
            model.model[0].conv = new_conv
            LOGGER.info("‚úÖ Student patched to 4 channels")
        
        return model
    
    def loss_with_distillation(self, batch, student_preds, teacher_preds):
        """Compute combined detection + distillation loss."""
        # Standard detection loss (from parent class)
        det_loss = self.compute_loss(batch, student_preds[:3] if isinstance(student_preds, tuple) else student_preds)
        
        # Distillation losses (simplified - using final predictions)
        if isinstance(student_preds, tuple) and isinstance(teacher_preds, tuple):
            # Logit distillation on detection outputs
            s_logits = student_preds[0] if len(student_preds) > 0 else student_preds
            t_logits = teacher_preds[0] if len(teacher_preds) > 0 else teacher_preds
            
            if isinstance(s_logits, torch.Tensor) and isinstance(t_logits, torch.Tensor):
                # Flatten and compute KL divergence
                s_flat = s_logits.view(s_logits.size(0), -1)
                t_flat = t_logits.view(t_logits.size(0), -1)
                
                # Match dimensions
                min_dim = min(s_flat.size(1), t_flat.size(1))
                s_flat = s_flat[:, :min_dim]
                t_flat = t_flat[:, :min_dim]
                
                logit_loss = self.distill_loss.logit_distillation(s_flat, t_flat)
            else:
                logit_loss = 0
        else:
            logit_loss = 0
        
        # Total loss
        total_loss = (
            self.distill_loss.alpha * det_loss[0] +
            self.distill_loss.beta * logit_loss
        )
        
        return total_loss, det_loss[1]


def create_distillation_config(teacher_scale, student_scale, teacher_weights=None):
    """Create training configuration for distillation."""
    configs = {
        '26x': ROOT / 'tracking/stage1/configs/yolo26x_rgbt.yaml',
        '26m': ROOT / 'tracking/stage1/configs/yolo26m_rgbt.yaml',
        '26s': ROOT / 'tracking/stage1/configs/yolo26s_rgbt.yaml',
        '26n': ROOT / 'tracking/stage1/configs/yolo26n_rgbt.yaml',
    }
    
    # Default weights if not provided
    default_weights = {
        '26x': ROOT / 'tracking/stage1/runs/vtmot_framework/v17_rgbt_yolo26x8/weights/best.pt',
        '26m': None,
        '26s': None,
        '26n': None,
    }
    
    return {
        'teacher_config': configs.get(teacher_scale),
        'teacher_weights': teacher_weights if teacher_weights else default_weights.get(teacher_scale),
        'student_config': configs.get(student_scale),
    }


def train_distillation(teacher_scale, student_scale, teacher_weights=None, epochs=30, batch=32, device=[0, 1]):
    """Run distillation training."""
    
    config = create_distillation_config(teacher_scale, student_scale, teacher_weights)
    
    if config['teacher_weights'] is None:
        LOGGER.error(f"‚ùå Teacher weights not found for {teacher_scale}. Please provide --teacher-weights.")
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    name = f"distill_{teacher_scale}_to_{student_scale}_{timestamp}"
    
    LOGGER.info("=" * 80)
    LOGGER.info(f"KNOWLEDGE DISTILLATION: {teacher_scale} ‚Üí {student_scale}")
    LOGGER.info("=" * 80)
    
    # Load student model from YAML
    student = YOLO(str(config['student_config']))
    
    # Training arguments
    args = {
        'data': str(ROOT / 'tracking/stage1/configs/vtmot_rgbt.yaml'),
        'epochs': epochs,
        'batch': batch,
        'imgsz': 640,
        'device': device,
        'project': str(ROOT / 'tracking/stage1/runs/distillation'),
        'name': name,
        'exist_ok': True,
        'pretrained': False,
        'optimizer': 'AdamW',
        'lr0': 0.001,
        'lrf': 0.01,
        'warmup_epochs': 3,
        'close_mosaic': 10,
        'amp': True,
        'val': True,
        'save': True,
        'save_period': 5,
    }
    
    # For now, use standard training (Ultralytics doesn't have built-in distillation)
    # The distillation logic would require custom trainer modification
    LOGGER.info("üìö Starting distillation training (standard mode with pretrained teacher features)...")
    
    # Train with teacher weights as starting point (partial transfer)
    if config['teacher_weights']:
        LOGGER.info(f"üîÑ Attempting to initialize student from teacher: {config['teacher_weights']}")
        args['pretrained'] = str(config['teacher_weights'])
    
    results = student.train(**args)
    
    LOGGER.info(f"‚úÖ Distillation complete: {name}")
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="RGBT Knowledge Distillation")
    parser.add_argument('--teacher', type=str, default='26x', choices=['26x', '26m', '26s'])
    parser.add_argument('--student', type=str, default='26m', choices=['26m', '26s', '26n'])
    parser.add_argument('--teacher-weights', type=str, default=None, help="Path to teacher weights")
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--batch', type=int, default=32)
    parser.add_argument('--device', type=str, default='0,1')
    
    args = parser.parse_args()
    
    device = [int(x) for x in args.device.split(',')]
    
    train_distillation(
        teacher_scale=args.teacher,
        student_scale=args.student,
        teacher_weights=args.teacher_weights,
        epochs=args.epochs,
        batch=args.batch,
        device=device
    )
