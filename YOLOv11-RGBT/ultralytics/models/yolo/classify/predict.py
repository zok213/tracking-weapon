# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import cv2
import numpy as np
import torch
from PIL import Image

from ultralytics.engine.predictor import BasePredictor
from ultralytics.engine.results import Results
from ultralytics.utils import DEFAULT_CFG, ops
import torchvision.transforms as T


class ClassificationPredictor(BasePredictor):
    """
    A class extending the BasePredictor class for prediction based on a classification model.
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """Initializes ClassificationPredictor setting the task to 'classify'."""
        super().__init__(cfg, overrides, _callbacks)
        self.args.task = "classify"
        self._legacy_transform_name = "ultralytics.yolo.data.augment.ToTensor"

        self.model_channels = self.args.channels

    def preprocess(self, img):
        """Converts input image to model-compatible data type."""
        if not isinstance(img, torch.Tensor):
            is_legacy_transform = any(
                self._legacy_transform_name in str(transform) for transform in self.transforms.transforms
            )

            if is_legacy_transform:  # to handle legacy transforms
                img = torch.stack([self.transforms(im) for im in img], dim=0)
            else:
                # ç›´æ¥å¤„ç†numpyæ•°ç»„ï¼Œé¿å…PILè½¬æ¢é—®é¢˜
                processed_imgs = []
                for im in img:
                    # ç¡®ä¿å›¾åƒæ˜¯numpyæ•°ç»„
                    if not isinstance(im, np.ndarray):
                        im = np.array(im)

                    # è·å–å½“å‰å›¾åƒçš„é€šé“æ•°
                    current_channels = im.shape[2] if len(im.shape) == 3 else 1

                    # ä¸¥æ ¼æ ¹æ®æ¨¡å‹é€šé“æ•°å¤„ç†å›¾åƒ
                    if self.model_channels == 1:
                        # 1é€šé“æ¨¡å‹ï¼šåªå¤„ç†1é€šé“å›¾åƒ
                        if len(im.shape) == 2:
                            im_processed = im  # ä¿æŒç°åº¦å›¾
                        elif current_channels == 1:
                            im_processed = im[:, :, 0]  # å•é€šé“å–ç¬¬ä¸€é€šé“
                        else:
                            # å¤šé€šé“è½¬ç°åº¦
                            im_processed = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)

                    elif self.model_channels == 3:
                        # 3é€šé“æ¨¡å‹ï¼šåªå¤„ç†3é€šé“å›¾åƒ
                        if len(im.shape) == 2:
                            # ç°åº¦å›¾è½¬3é€šé“
                            im_processed = np.stack([im] * 3, axis=2)
                        elif current_channels == 1:
                            # å•é€šé“è½¬3é€šé“
                            im_processed = np.repeat(im, 3, axis=2)
                        elif current_channels == 3:
                            # 3é€šé“ä¿æŒåŸæ ·
                            im_processed = im
                        elif current_channels == 4:
                            # 4é€šé“å–å‰3é€šé“
                            im_processed = im[:, :, :3]
                        elif current_channels == 6:
                            # 6é€šé“å–å‰3é€šé“
                            im_processed = im[:, :, :3]
                        else:
                            # å…¶ä»–é€šé“æ•°å–å‰3é€šé“
                            im_processed = im[:, :, :3]

                    elif self.model_channels == 4:
                        # 4é€šé“æ¨¡å‹ï¼šåªå¤„ç†4é€šé“å›¾åƒ
                        if len(im.shape) == 2:
                            # ç°åº¦å›¾æ— æ³•è½¬æ¢ä¸º4é€šé“ï¼ŒæŠ¥é”™
                            raise ValueError("1-channel image cannot be converted to 4 channels for 4-channel model")
                        elif current_channels == 1:
                            # å•é€šé“æ— æ³•è½¬æ¢ä¸º4é€šé“ï¼ŒæŠ¥é”™
                            raise ValueError("1-channel image cannot be converted to 4 channels for 4-channel model")
                        elif current_channels == 3:
                            # 3é€šé“æ— æ³•è½¬æ¢ä¸º4é€šé“ï¼ŒæŠ¥é”™
                            raise ValueError("3-channel image cannot be converted to 4 channels for 4-channel model")
                        elif current_channels == 4:
                            # 4é€šé“ä¿æŒåŸæ ·
                            im_processed = im
                        elif current_channels == 6:
                            # 6é€šé“å–å‰4é€šé“
                            im_processed = im[:, :, :4]
                        else:
                            # å…¶ä»–é€šé“æ•°å–å‰4é€šé“
                            im_processed = im[:, :, :min(current_channels, 4)]
                            if current_channels < 4:
                                raise ValueError(
                                    f"{current_channels}-channel image cannot be converted to 4 channels for 4-channel model")

                    elif self.model_channels == 6:
                        # 6é€šé“æ¨¡å‹ï¼šåªå¤„ç†6é€šé“å›¾åƒ
                        if len(im.shape) == 2:
                            # ç°åº¦å›¾æ— æ³•è½¬æ¢ä¸º6é€šé“ï¼ŒæŠ¥é”™
                            raise ValueError("1-channel image cannot be converted to 6 channels for 6-channel model")
                        elif current_channels == 1:
                            # å•é€šé“æ— æ³•è½¬æ¢ä¸º6é€šé“ï¼ŒæŠ¥é”™
                            raise ValueError("1-channel image cannot be converted to 6 channels for 6-channel model")
                        elif current_channels == 3:
                            # 3é€šé“æ— æ³•è½¬æ¢ä¸º6é€šé“ï¼ŒæŠ¥é”™
                            raise ValueError("3-channel image cannot be converted to 6 channels for 6-channel model")
                        elif current_channels == 4:
                            # 4é€šé“æ— æ³•è½¬æ¢ä¸º6é€šé“ï¼ŒæŠ¥é”™
                            raise ValueError("4-channel image cannot be converted to 6 channels for 6-channel model")
                        elif current_channels == 6:
                            # 6é€šé“ä¿æŒåŸæ ·
                            im_processed = im
                        else:
                            # å…¶ä»–é€šé“æ•°å–å‰6é€šé“
                            im_processed = im[:, :, :min(current_channels, 6)]
                            if current_channels < 6:
                                raise ValueError(
                                    f"{current_channels}-channel image cannot be converted to 6 channels for 6-channel model")

                    else:
                        # å…¶ä»–é€šé“æ•°æ¨¡å‹ï¼šä¸¥æ ¼åŒ¹é…
                        if current_channels != self.model_channels:
                            raise ValueError(
                                f"{current_channels}-channel image does not match {self.model_channels}-channel model")
                        im_processed = im

                    # åº”ç”¨å‡ ä½•å˜æ¢
                    im_processed = self._apply_geometric_transforms(im_processed)

                    # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–åˆ°[0,1]
                    tensor_img = torch.from_numpy(im_processed.transpose(2, 0, 1)).float() / 255.0

                    # åº”ç”¨å½’ä¸€åŒ–
                    tensor_img = self._apply_normalization(tensor_img)

                    processed_imgs.append(tensor_img)

                img = torch.stack(processed_imgs, dim=0)

        img = (img if isinstance(img, torch.Tensor) else torch.from_numpy(img)).to(self.model.device)
        return img.half() if self.model.fp16 else img.float()

    def _apply_geometric_transforms(self, img):
        """åº”ç”¨å‡ ä½•å˜æ¢ï¼ˆresizeå’Œcentercropï¼‰"""
        # è·å–transformå‚æ•°
        import torchvision.transforms as T

        # æŸ¥æ‰¾resizeå‚æ•°
        resize_size = 224  # é»˜è®¤å€¼
        for transform in self.transforms.transforms:
            if isinstance(transform, T.Resize):
                if isinstance(transform.size, int):
                    resize_size = (transform.size, transform.size)
                else:
                    resize_size = transform.size
                break

        # åº”ç”¨resize
        h, w = img.shape[:2]
        if isinstance(resize_size, int):
            resize_size = (resize_size, resize_size)

        if (h, w) != resize_size:
            # æ ¹æ®æ¨¡å‹é€šé“æ•°é‡‡ç”¨ä¸åŒçš„resizeç­–ç•¥
            if self.model_channels == 1:
                # 1é€šé“ï¼šç°åº¦resize
                img = cv2.resize(img, resize_size, interpolation=cv2.INTER_LINEAR)
            elif self.model_channels == 3:
                # 3é€šé“ï¼šå½©è‰²resize
                img = cv2.resize(img, resize_size, interpolation=cv2.INTER_LINEAR)
            elif self.model_channels == 4:
                # 4é€šé“ï¼šå‰3é€šé“å½©è‰²resizeï¼Œç¬¬4é€šé“ç°åº¦resize
                rgb_resized = cv2.resize(img[:, :, :3], resize_size, interpolation=cv2.INTER_LINEAR)
                fourth_resized = cv2.resize(img[:, :, 3], resize_size, interpolation=cv2.INTER_LINEAR)
                img = np.concatenate([rgb_resized, fourth_resized[:, :, np.newaxis]], axis=2)
            elif self.model_channels == 6:
                # 6é€šé“ï¼šåˆ†åˆ«å¤„ç†ä¸¤ä¸ªRGBç»„
                rgb1_resized = cv2.resize(img[:, :, :3], resize_size, interpolation=cv2.INTER_LINEAR)
                rgb2_resized = cv2.resize(img[:, :, 3:6], resize_size, interpolation=cv2.INTER_LINEAR)
                img = np.concatenate([rgb1_resized, rgb2_resized], axis=2)
            else:
                # å…¶ä»–é€šé“æ•°ï¼šæ ‡å‡†resize
                img = cv2.resize(img, resize_size, interpolation=cv2.INTER_LINEAR)

        # æŸ¥æ‰¾centercropå‚æ•°
        for transform in self.transforms.transforms:
            if isinstance(transform, T.CenterCrop):
                crop_size = transform.size
                if isinstance(crop_size, int):
                    crop_size = (crop_size, crop_size)

                h, w = img.shape[:2]
                if h >= crop_size[0] and w >= crop_size[1]:
                    top = (h - crop_size[0]) // 2
                    left = (w - crop_size[1]) // 2
                    img = img[top:top + crop_size[0], left:left + crop_size[1]]
                break

        return img

    def _apply_normalization(self, tensor):
        """åº”ç”¨å½’ä¸€åŒ–"""
        # æŸ¥æ‰¾å½’ä¸€åŒ–å‚æ•°
        for transform in self.transforms.transforms:
            if isinstance(transform, T.Normalize):
                mean = transform.mean
                std = transform.std

                # æ ¹æ®æ¨¡å‹é€šé“æ•°ä¸¥æ ¼åº”ç”¨å½’ä¸€åŒ–
                tensor_channels = tensor.shape[0]

                if tensor_channels == len(mean):
                    # é€šé“æ•°å®Œå…¨åŒ¹é…ï¼Œç›´æ¥åº”ç”¨
                    for t, m, s in zip(tensor, mean, std):
                        t.sub_(m).div_(s)
                elif len(mean) == 3 and tensor_channels != 3:
                    # 3é€šé“å½’ä¸€åŒ–å‚æ•°åº”ç”¨åˆ°å¤šé€šé“æ¨¡å‹
                    if tensor_channels == 1:
                        # 1é€šé“æ¨¡å‹ï¼šå–å¹³å‡å€¼
                        avg_mean = sum(mean) / 3.0
                        avg_std = sum(std) / 3.0
                        tensor.sub_(avg_mean).div_(avg_std)
                    elif tensor_channels == 4:
                        # 4é€šé“æ¨¡å‹ï¼šå‰3é€šé“ç”¨RGBå‚æ•°ï¼Œç¬¬4é€šé“ç”¨é»˜è®¤
                        for i in range(3):
                            tensor[i].sub_(mean[i]).div_(std[i])
                        tensor[3].sub_(0.0).div_(1.0)
                    elif tensor_channels == 6:
                        # 6é€šé“æ¨¡å‹ï¼šä¸¤ç»„RGBåˆ†åˆ«åº”ç”¨ç›¸åŒçš„å½’ä¸€åŒ–
                        for i in range(3):
                            tensor[i].sub_(mean[i]).div_(std[i])  # ç¬¬ä¸€ç»„RGB
                            tensor[i + 3].sub_(mean[i]).div_(std[i])  # ç¬¬äºŒç»„RGB
                else:
                    # ä¸åŒ¹é…çš„æƒ…å†µï¼šä½¿ç”¨é»˜è®¤å½’ä¸€åŒ–
                    for i in range(tensor_channels):
                        if i < len(mean):
                            tensor[i].sub_(mean[i]).div_(std[i])
                        else:
                            tensor[i].sub_(0.0).div_(1.0)
                break

        return tensor

    def postprocess(self, preds, img, orig_imgs):
        """Post-processes predictions to return Results objects."""
        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list
            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)

        preds = preds[0] if isinstance(preds, (list, tuple)) else preds

        # å¤„ç†å¤šé€šé“å›¾åƒçš„å¯è§†åŒ–é—®é¢˜
        processed_orig_imgs = []
        for orig_img in orig_imgs:
            # å¦‚æœåŸå§‹å›¾åƒæ˜¯å¤šé€šé“ï¼ˆ>3ï¼‰ï¼Œè½¬æ¢ä¸º3é€šé“ç”¨äºå¯è§†åŒ–
            if len(orig_img.shape) == 3 and orig_img.shape[2] > 3:
                if orig_img.shape[2] == 4:
                    # 4é€šé“ï¼šå–å‰3é€šé“ç”¨äºå¯è§†åŒ–
                    vis_img = orig_img[:, :, :3]
                elif orig_img.shape[2] == 6:
                    # 6é€šé“ï¼šå–ç¬¬ä¸€ç»„RGBç”¨äºå¯è§†åŒ–
                    vis_img = orig_img[:, :, :3]
                else:
                    # å…¶ä»–å¤šé€šé“ï¼šå–å‰3é€šé“
                    vis_img = orig_img[:, :, :3]
                processed_orig_imgs.append(vis_img)
            else:
                processed_orig_imgs.append(orig_img)

        return [
            Results(orig_img, path=img_path, names=self.model.names, probs=pred)
            for pred, orig_img, img_path in zip(preds, processed_orig_imgs, self.batch[0])
        ]