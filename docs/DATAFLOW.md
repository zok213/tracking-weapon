# ðŸ”„ Vietnam Weapon Detection System - Complete Dataflow v7.0

## System Pipeline Overview

This document provides the complete end-to-end dataflow for detecting **persons carrying weapons** from nadir drone footage, with full **gimbal hub integration**.

---

## COMPLETE SYSTEM DATAFLOW

```
================================================================================
                        INPUT PIPELINE (Gimbal Hub)
================================================================================
    â†“
Video Stream: Nadir drone (50-70m altitude, 1920Ã—1080 @ 30 FPS)
â”œâ”€ RGB channel: Standard visible spectrum (HDMI â†’ Gimbal Hub)
â”œâ”€ Thermal channel: Synchronized IR (640Ã—512 @ 30 FPS, Gremsy thermal â†’ USB)
â””â”€ Laser rangefinder: Distance measurement (Serial â†’ Gimbal Hub)
    â†“
Gimbal Hub (Gremsy Hub) - Central Data Multiplexer:
â”œâ”€ Power distribution: 12V @ 3A (gimbal/sensors), 5V @ 2A (logic)
â”œâ”€ Signal multiplexing: Route RGB (HDMI), Thermal (USB), Laser (Serial)
â”œâ”€ Frame synchronization: Align RGB + Thermal timestamps (Â±1ms tolerance)
â”œâ”€ UART bridge: Convert laser serial to processable format (115200 baud)
â””â”€ Output to Jetson: USB3 video streams + Serial data + MAVLink telemetry @ 50Hz
    â†“
================================================================================
                            PREPROCESSING
================================================================================
    â†“
â”œâ”€ Receive synced frames from Gimbal Hub:
â”‚  â”œâ”€ RGB frame: 1920Ã—1080 with timestamp
â”‚  â”œâ”€ Thermal frame: 640Ã—512 with timestamp
â”‚  â”œâ”€ Frame sync drift: Must be â‰¤1ms
â”‚  â””â”€ Sequence ID: Unique frame identifier
â”‚
â”œâ”€ Resize RGB: 1920Ã—1080 â†’ 640Ã—640 (letterbox, maintain aspect ratio)
â”œâ”€ Resize Thermal: 640Ã—512 â†’ 640Ã—640 (bilinear interpolation, match RGB)
â”œâ”€ Normalize: RGB [0-255] â†’ [0-1], Thermal [0-255] â†’ [0-1]
â”œâ”€ Channel stack: RGB (3ch) + Thermal (1ch replicated to 3ch for backbone)
â””â”€ Synchronize: Frame-level alignment verified (timestamp matching from Hub)
    â†“
Preprocessing output:
â”œâ”€ rgb_tensor: [1, 3, 640, 640] float32
â”œâ”€ thermal_tensor: [1, 3, 640, 640] float32
â”œâ”€ frame_metadata: {timestamp_ms, seq_id, laser_distance, gimbal_angles}
â””â”€ Latency: 3ms
    â†“
================================================================================
                    DETECTION BACKBONE (YOLO26n + CBAM Fusion)
================================================================================
    â†“
â”œâ”€ RGB branch (Primary detection):
â”‚  â”œâ”€ Input: 640Ã—640 RGB image [1, 3, 640, 640]
â”‚  â”œâ”€ Backbone: 8 convolutional stages (CSP-Darknet style)
â”‚  â”‚  â”œâ”€ Stage 1: Conv2d(3â†’16, k=3, s=2) â†’ 320Ã—320
â”‚  â”‚  â”œâ”€ Stage 2: C3k2 block â†’ 160Ã—160
â”‚  â”‚  â”œâ”€ Stage 3: C3k2 block â†’ 80Ã—80 (P3 features)
â”‚  â”‚  â”œâ”€ Stage 4: C3k2 block â†’ 40Ã—40 (P4 features)
â”‚  â”‚  â”œâ”€ Stage 5-8: C3k2 blocks â†’ 20Ã—20 (P5 features)
â”‚  â”‚  â””â”€ Total params: ~5.5M (YOLOv12n lightweight)
â”‚  â”‚
â”‚  â”œâ”€ FPN layers (Feature Pyramid Network):
â”‚  â”‚  â”œâ”€ P3: 80Ã—80Ã—256 (small objects - weapons at distance)
â”‚  â”‚  â”œâ”€ P4: 40Ã—40Ã—512 (medium objects - persons)
â”‚  â”‚  â””â”€ P5: 20Ã—20Ã—1024 (large objects - vehicles)
â”‚  â”‚
â”‚  â””â”€ Output: Multi-scale feature maps for detection head
â”‚
â””â”€ Thermal branch (CBAM Early Fusion):
   â”œâ”€ Input: 640Ã—640 thermal image [1, 3, 640, 640]
   â”œâ”€ Lightweight backbone: 5 conv stages (~1.5M params, 30% of RGB)
   â”‚  â”œâ”€ Designed for thermal-specific patterns
   â”‚  â”œâ”€ Focus on temperature gradients and cold spots
   â”‚  â””â”€ Output: Thermal features at P3 level only (small objects)
   â”‚
   â”œâ”€ Fusion point: After backbone, at P3 layer (small object focus)
   â”œâ”€ CBAM Fusion mechanism:
   â”‚  â”œâ”€ Channel attention (WHAT to focus on):
   â”‚  â”‚  â”œâ”€ Global avg pool + max pool â†’ channel descriptors
   â”‚  â”‚  â”œâ”€ FC(256â†’16â†’256) â†’ channel weights
   â”‚  â”‚  â”œâ”€ Learn: Which modality to trust per channel
   â”‚  â”‚  â””â”€ Daytime: 70% RGB / 30% Thermal
   â”‚  â”‚      Night: 40% RGB / 60% Thermal
   â”‚  â”‚
   â”‚  â””â”€ Spatial attention (WHERE to focus):
   â”‚     â”œâ”€ Channel-wise avg + max â†’ spatial map
   â”‚     â”œâ”€ Conv(2â†’1, k=7) â†’ spatial weights
   â”‚     â”œâ”€ Highlight cold spots (weapon metal signatures)
   â”‚     â””â”€ Suppress warm body regions (less useful for weapons)
   â”‚
   â””â”€ Output: Fused features [1, 256, 80, 80] ready for detection
    â†“
Backbone latency breakdown:
â”œâ”€ RGB backbone: 7ms
â”œâ”€ Thermal branch: 2ms
â”œâ”€ CBAM fusion: 1ms
â””â”€ Total: 10ms
    â†“
================================================================================
                    DETECTION HEAD (Person + Weapon Detection)
================================================================================
    â†“
â”œâ”€ Input: Fused features from P3 (80Ã—80), P4 (40Ã—40), P5 (20Ã—20) layers
â”‚
â”œâ”€ Detection classes (4 total):
â”‚  â”œâ”€ Class 0: person (rider on motorcycle - primary target context)
â”‚  â”œâ”€ Class 1: knife_machete (melee blades - hard class)
â”‚  â”œâ”€ Class 2: metal_rod (crowbars, pipes - easier class)
â”‚  â””â”€ Class 3: motorcycle (context for rider detection)
â”‚
â”œâ”€ Prediction grid per scale:
â”‚  â”œâ”€ P3 (stride 8): 80Ã—80 grid = 6,400 cells â†’ weapons (5-30 pixels)
â”‚  â”œâ”€ P4 (stride 16): 40Ã—40 grid = 1,600 cells â†’ persons (30-100 pixels)
â”‚  â””â”€ P5 (stride 32): 20Ã—20 grid = 400 cells â†’ vehicles (100+ pixels)
â”‚
â”œâ”€ Outputs per grid cell:
â”‚  â”œâ”€ Bounding box: 4 coords (x_center, y_center, width, height) normalized
â”‚  â”œâ”€ Objectness score: Is there an object? [0-1]
â”‚  â”œâ”€ Class probabilities: [person, knife, rod, motorcycle]
â”‚  â””â”€ Confidence: objectness Ã— class_prob
â”‚
â”œâ”€ Person-Weapon Association:
â”‚  â”œâ”€ Detect person bbox (rider on motorcycle)
â”‚  â”œâ”€ Detect weapon bbox (knife/rod near person)
â”‚  â”œâ”€ Associate if: IoU(person, weapon) > 0.1 OR
â”‚  â”‚              distance(person_center, weapon_center) < 50px
â”‚  â”œâ”€ Result: weapon_owner_id linking weapon to specific person
â”‚  â””â”€ Benefit: Track "Person A carrying Knife" not just "Knife"
â”‚
â””â”€ Output: Raw detections [N Ã— (x, y, w, h, objectness, class_probs[4])]
    â†“
Detection head latency: 3ms
    â†“
================================================================================
                POST-PROCESSING (Class-Specific Filtering)
================================================================================
    â†“
â”œâ”€ Confidence thresholds (class-specific):
â”‚  â”œâ”€ person: Keep if conf â‰¥ 0.50 (easier to detect)
â”‚  â”œâ”€ knife_machete: Keep if conf â‰¥ 0.68 (stricter for hard class)
â”‚  â”œâ”€ metal_rod: Keep if conf â‰¥ 0.65 (slightly easier than knife)
â”‚  â””â”€ motorcycle: Keep if conf â‰¥ 0.50 (context only)
â”‚
â”œâ”€ NMS-Free (YOLO26):
â”‚  â”œâ”€ No IoU threshold needed
â”‚  â””â”€ End-to-End detection output
â”‚
â”œâ”€ Person-Weapon Linking:
â”‚  â”œâ”€ For each weapon detection:
â”‚  â”‚  â”œâ”€ Find nearest person detection
â”‚  â”‚  â”œâ”€ Compute spatial relationship:
â”‚  â”‚  â”‚  â”œâ”€ IoU overlap (weapon on person's body)
â”‚  â”‚  â”‚  â”œâ”€ Distance (weapon near person)
â”‚  â”‚  â”‚  â””â”€ Position (weapon at hip/back/rack area)
â”‚  â”‚  â”œâ”€ If linked: weapon.owner_id = person.track_id
â”‚  â”‚  â””â”€ If unlinked: weapon.owner_id = None (standalone weapon)
â”‚  â”‚
â”‚  â””â”€ Benefits:
â”‚     â”œâ”€ Reduces false positives (weapons without persons may be bike parts)
â”‚     â”œâ”€ Enables "Person A carrying Machete" alerts
â”‚     â””â”€ Supports legal evidence ("Suspect in red shirt with crowbar")
â”‚
â””â”€ Output: Filtered detections with owner associations
   [N Ã— {bbox, class_id, confidence, owner_id}]
    â†“
Latency: <1ms
    â†“
================================================================================
            THERMAL CONFIDENCE FUSION (Physics-Based Verification)
================================================================================
    â†“
â”œâ”€ For each weapon detection:
â”‚  â”œâ”€ Extract thermal ROI (crop thermal frame at weapon bbox)
â”‚  â”œâ”€ Pad bbox by 20% (capture surrounding context)
â”‚  â”‚
â”‚  â”œâ”€ Compute 5 hand-crafted thermal features:
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ F1: Temperature Gradient (Edge Sharpness)
â”‚  â”‚  â”‚  â”œâ”€ Formula: |âˆ‡T| = sqrt((dT/dx)Â² + (dT/dy)Â²)
â”‚  â”‚  â”‚  â”œâ”€ Weapon signature: 10-30 Â°C/cm (sharp metal edges)
â”‚  â”‚  â”‚  â”œâ”€ Non-weapon: 1-5 Â°C/cm (smooth gradients)
â”‚  â”‚  â”‚  â””â”€ Normalized score: gradient / 30.0
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ F2: Cold Spot Ratio
â”‚  â”‚  â”‚  â”œâ”€ Formula: count(T < body_temp - 5Â°C) / total_pixels
â”‚  â”‚  â”‚  â”œâ”€ Weapon signature: 30-60% cold pixels (metal absorbs cold)
â”‚  â”‚  â”‚  â”œâ”€ Non-weapon: <20% cold pixels
â”‚  â”‚  â”‚  â””â”€ Normalized score: ratio / 0.6
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ F3: Temperature Contrast
â”‚  â”‚  â”‚  â”œâ”€ Formula: max_temp - min_temp in ROI
â”‚  â”‚  â”‚  â”œâ”€ Weapon signature: 15-35Â°C (cold metal vs warm body)
â”‚  â”‚  â”‚  â”œâ”€ Non-weapon: <10Â°C contrast
â”‚  â”‚  â”‚  â””â”€ Normalized score: contrast / 35.0
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ F4: Spatial Concentration
â”‚  â”‚  â”‚  â”œâ”€ Find center of mass of cold pixels
â”‚  â”‚  â”‚  â”œâ”€ Compute variance of cold pixel positions
â”‚  â”‚  â”‚  â”œâ”€ Weapon signature: Low variance (compact cold cluster)
â”‚  â”‚  â”‚  â”œâ”€ Non-weapon: High variance (scattered)
â”‚  â”‚  â”‚  â””â”€ Normalized score: 1.0 - (variance / max_variance)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ F5: Temporal Consistency
â”‚  â”‚     â”œâ”€ Compare features with previous 5 frames
â”‚  â”‚     â”œâ”€ Compute cosine similarity
â”‚  â”‚     â”œâ”€ Weapon signature: >0.8 correlation (stable across frames)
â”‚  â”‚     â”œâ”€ Non-weapon: <0.5 correlation (changing patterns)
â”‚  â”‚     â””â”€ Normalized score: correlation coefficient
â”‚  â”‚
â”‚  â”œâ”€ 30-Frame Buffer for LSTM:
â”‚  â”‚  â”œâ”€ Stack features: [30 Ã— 5] tensor per track
â”‚  â”‚  â”œâ”€ LSTM model: LSTM(input=5, hidden=32) â†’ Dense(16) â†’ Dense(1) â†’ Sigmoid
â”‚  â”‚  â”œâ”€ Output: thermal_confidence âˆˆ [0, 1]
â”‚  â”‚  â””â”€ Latency: 2ms
â”‚  â”‚
â”‚  â”œâ”€ Confidence Fusion Decision:
â”‚  â”‚  â”œâ”€ Rule 1: If (RGB_conf > 0.7) AND (thermal_conf > 0.6):
â”‚  â”‚  â”‚  â””â”€ combined_conf = 0.95 (HIGH confidence, both agree)
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Rule 2: If (RGB_conf > 0.7) AND (thermal_conf > 0.3):
â”‚  â”‚  â”‚  â””â”€ combined_conf = 0.75 + 0.15 Ã— thermal_conf
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Rule 3: If (thermal_conf < 0.3):
â”‚  â”‚  â”‚  â””â”€ combined_conf = RGB_conf Ã— 0.85 (thermal doesn't support)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Default: weighted_avg = 0.7 Ã— RGB_conf + 0.3 Ã— thermal_conf
â”‚  â”‚
â”‚  â””â”€ Confidence boost summary:
â”‚     â”œâ”€ metal_rod (pipe/crowbar): +10-15% boost (strong thermal signature)
â”‚     â”œâ”€ knife_machete: +5-8% boost (weaker thermal, smaller object)
â”‚     â””â”€ No thermal ROI available: Keep RGB confidence unchanged
â”‚
â””â”€ Output: Enhanced detections with thermal verification
   [N Ã— {bbox, class_id, combined_conf, thermal_conf, owner_id}]
    â†“
Thermal fusion latency: 2ms
    â†“
================================================================================
                MULTI-OBJECT TRACKING (ByteTrack Algorithm)
================================================================================
    â†“
â”œâ”€ Input: Filtered detections from frame N
â”‚  [N Ã— {bbox, class_id, combined_conf, owner_id}]
â”‚
â”œâ”€ ByteTrack Two-Stage Association:
â”‚  â”‚
â”‚  â”œâ”€ STAGE 1: High-Confidence Association (conf â‰¥ 0.70)
â”‚  â”‚  â”œâ”€ Get all detections with confidence â‰¥ 0.70
â”‚  â”‚  â”œâ”€ Get all ACTIVE tracklets (matched in recent frames)
â”‚  â”‚  â”œâ”€ Compute IoU distance matrix:
â”‚  â”‚  â”‚  cost[i,j] = 1.0 - IoU(track[i].bbox, detection[j].bbox)
â”‚  â”‚  â”œâ”€ Apply Hungarian algorithm (optimal linear assignment)
â”‚  â”‚  â”œâ”€ Match threshold: IoU â‰¥ 0.50 (cost â‰¤ 0.50)
â”‚  â”‚  â”œâ”€ Results:
â”‚  â”‚  â”‚  â”œâ”€ Matched tracks: Update state with new detection
â”‚  â”‚  â”‚  â”œâ”€ Unmatched tracks: Go to Stage 2
â”‚  â”‚  â”‚  â””â”€ Unmatched detections: Create new tracks
â”‚  â”‚  â””â”€ Handle gimbal motion compensation (subtract gimbal shift from tracks)
â”‚  â”‚
â”‚  â””â”€ STAGE 2: Low-Confidence Recovery (0.30 â‰¤ conf < 0.70)
â”‚     â”œâ”€ Purpose: Recover tracks through brief occlusions or missed detections
â”‚     â”œâ”€ Get detections with 0.30 â‰¤ confidence < 0.70
â”‚     â”œâ”€ Get UNMATCHED tracklets from Stage 1
â”‚     â”œâ”€ Compute IoU distance matrix
â”‚     â”œâ”€ Apply Hungarian algorithm
â”‚     â”œâ”€ Results:
â”‚     â”‚  â”œâ”€ Matched: Recover track (occlusion handled!)
â”‚     â”‚  â””â”€ Unmatched tracks: Keep in buffer for 30 frames
â”‚     â””â”€ KEY INSIGHT: Low-conf detections often occur during occlusion
â”‚
â”œâ”€ Track State Machine:
â”‚  â”œâ”€ NEW â†’ TENTATIVE: Created from unmatched high-conf detection
â”‚  â”œâ”€ TENTATIVE â†’ ACTIVE: After 3 consecutive frame matches
â”‚  â”œâ”€ ACTIVE â†’ ACTIVE: Matched in current frame
â”‚  â”œâ”€ ACTIVE â†’ LOST: Unmatched for 1+ frames (Kalman prediction)
â”‚  â”œâ”€ LOST â†’ ACTIVE: Re-matched within 30 frames
â”‚  â””â”€ LOST â†’ DELETED: Unmatched for 30 consecutive frames
â”‚
â”œâ”€ Kalman Filter (per track):
â”‚  â”œâ”€ State vector: [x, y, w, h, vx, vy, vw, vh]
â”‚  â”œâ”€ Predict: Estimate next bbox using velocity
â”‚  â”œâ”€ Update: Correct estimate when detection matches
â”‚  â”œâ”€ Gimbal compensation: Subtract gimbal motion from velocity
â”‚  â””â”€ Handles: Brief detection dropouts, motion blur
â”‚
â”œâ”€ Person-Weapon Track Linking:
â”‚  â”œâ”€ Maintain association from detection phase
â”‚  â”œâ”€ If weapon.owner_id != None:
â”‚  â”‚  â”œâ”€ Link weapon track to person track
â”‚  â”‚  â”œâ”€ Inherit person's GPS trajectory
â”‚  â”‚  â””â”€ Alert shows: "Person Track #42 carrying Machete Track #87"
â”‚  â””â”€ If person moves, weapon follows (spatial consistency check)
â”‚
â”œâ”€ Track output per frame:
â”‚  â”œâ”€ track_id: Unique identifier (persistent across video)
â”‚  â”œâ”€ bbox: Current position [x, y, w, h]
â”‚  â”œâ”€ class_id: 0=person, 1=knife, 2=rod, 3=motorcycle
â”‚  â”œâ”€ confidence: Smoothed over last 10 frames
â”‚  â”œâ”€ age: Frames since track creation
â”‚  â”œâ”€ owner_id: Person track ID (for weapons)
â”‚  â””â”€ state: ACTIVE, LOST, TENTATIVE
â”‚
â””â”€ Tracking output:
   [M Ã— {track_id, bbox, class_id, confidence, age, owner_id, state}]
    â†“
Tracking latency: 2ms
    â†“
================================================================================
        GIMBAL CONTROL (Gremsy MAVLink Protocol via Gimbal Hub)
================================================================================
    â†“
â”œâ”€ Gimbal Hub Interface:
â”‚  â”œâ”€ Receive: Synced frames + telemetry from hub
â”‚  â”œâ”€ Send: Gimbal commands through hub's UART bridge
â”‚  â”œâ”€ Protocol: MAVLink v2 at 115200 baud
â”‚  â””â”€ Update rate: 50Hz (20ms command interval)
â”‚
â”œâ”€ Trigger Conditions for Gimbal Action:
â”‚  â”‚
â”‚  â”œâ”€ Condition 1: Low confidence + small object
â”‚  â”‚  â”œâ”€ If (track_confidence < 0.65) AND (bbox_width < 25px):
â”‚  â”‚  â”‚  â””â”€ Object too small and uncertain â†’ Zoom for clarity
â”‚  â”‚  â”œâ”€ Action: Center on target + Zoom 8Ã—
â”‚  â”‚  â””â”€ Goal: Get better resolution for re-detection
â”‚  â”‚
â”‚  â”œâ”€ Condition 2: New high-value detection
â”‚  â”‚  â”œâ”€ If (class == knife_machete) AND (first detection):
â”‚  â”‚  â”‚  â””â”€ Knife is hard class, needs confirmation
â”‚  â”‚  â”œâ”€ Action: Center on target + Zoom 4Ã—
â”‚  â”‚  â””â”€ Goal: Verify blade shape in higher resolution
â”‚  â”‚
â”‚  â””â”€ Condition 3: Track losing confidence
â”‚     â”œâ”€ If (confidence dropping 3 frames) AND (still ACTIVE):
â”‚     â”‚  â””â”€ Object becoming occluded or blurred
â”‚     â”œâ”€ Action: Center on last known position + Zoom 4Ã—
â”‚     â””â”€ Goal: Maintain tracking through occlusion
â”‚
â”œâ”€ Centering Algorithm:
â”‚  â”œâ”€ Target: Weapon bbox center (x_center, y_center)
â”‚  â”œâ”€ Frame center: (320, 256) at 640Ã—512 thermal / (960, 540) at 1920Ã—1080 RGB
â”‚  â”œâ”€ Error vector: error = (target - frame_center)
â”‚  â”‚
â”‚  â”œâ”€ Pixel to Angle conversion:
â”‚  â”‚  â”œâ”€ Horizontal FOV: 60Â° (typical gimbal camera)
â”‚  â”‚  â”œâ”€ Vertical FOV: 45Â°
â”‚  â”‚  â”œâ”€ yaw_angle = error_x Ã— (60Â° / frame_width)
â”‚  â”‚  â”œâ”€ pitch_angle = error_y Ã— (45Â° / frame_height)
â”‚  â”‚  â””â”€ Apply PID smoothing: Kp=0.5, Kd=0.1 (prevent oscillation)
â”‚  â”‚
â”‚  â”œâ”€ MAVLink Command (via Gimbal Hub):
â”‚  â”‚  â”œâ”€ MAV_CMD_DO_MOUNT_CONTROL (205)
â”‚  â”‚  â”‚  â”œâ”€ param1: pitch_angle (degrees)
â”‚  â”‚  â”‚  â”œâ”€ param2: roll_angle (0, stabilized)
â”‚  â”‚  â”‚  â”œâ”€ param3: yaw_angle (degrees)
â”‚  â”‚  â”‚  â””â”€ param7: MAV_MOUNT_MODE_MAVLINK_TARGETING
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Command flow: Jetson â†’ Hub â†’ Gimbal (via UART)
â”‚  â”‚
â”‚  â””â”€ Centering tolerance: Â±2-3 pixels (stop adjusting when centered)
â”‚
â”œâ”€ Adaptive Zoom Control:
â”‚  â”œâ”€ Zoom level selection based on target size:
â”‚  â”‚  â”œâ”€ bbox_width <  15px: Zoom 16Ã— (maximum, very small object)
â”‚  â”‚  â”œâ”€ bbox_width <  30px: Zoom 8Ã— (small object, typical for weapons)
â”‚  â”‚  â”œâ”€ bbox_width <  60px: Zoom 4Ã— (medium object)
â”‚  â”‚  â”œâ”€ bbox_width < 100px: Zoom 2Ã— (larger object)
â”‚  â”‚  â””â”€ bbox_width â‰¥ 100px: Zoom 1Ã— (no zoom needed)
â”‚  â”‚
â”‚  â”œâ”€ Zoom command (MAVLink):
â”‚  â”‚  â”œâ”€ MAV_CMD_SET_CAMERA_ZOOM (531)
â”‚  â”‚  â”‚  â”œâ”€ param1: ZOOM_TYPE_CONTINUOUS (0)
â”‚  â”‚  â”‚  â””â”€ param2: zoom_level (1.0 - 20.0)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Dwell time: Wait 150-200ms for optical refocus
â”‚  â”‚
â”‚  â””â”€ Effective resolution at zoom:
â”‚     â”œâ”€ 1Ã—: 1920Ã—1080 â†’ weapon ~10-20px
â”‚     â”œâ”€ 4Ã—: 7680Ã—4320 effective â†’ weapon ~40-80px
â”‚     â”œâ”€ 8Ã—: 15360Ã—8640 effective â†’ weapon ~80-160px
â”‚     â””â”€ 16Ã—: 30720Ã—17280 effective â†’ weapon ~160-320px (very clear!)
â”‚
â”œâ”€ Re-detection at Higher Zoom:
â”‚  â”œâ”€ After zoom stabilizes (200ms delay):
â”‚  â”‚  â”œâ”€ Capture zoomed frame from Gimbal Hub
â”‚  â”‚  â”œâ”€ Preprocess: Crop center 640Ã—640 (zoomed region only)
â”‚  â”‚  â”œâ”€ Run YOLOv12n inference on zoomed image
â”‚  â”‚  â””â”€ Higher resolution = better feature visibility
â”‚  â”‚
â”‚  â”œâ”€ Confirmation logic:
â”‚  â”‚  â”œâ”€ If re-detected with conf â‰¥ 0.70:
â”‚  â”‚  â”‚  â””â”€ Boost original confidence +20% (zoom confirmed!)
â”‚  â”‚  â”‚  â””â”€ Alert level: Upgrade to VERY_HIGH
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ If re-detected with conf < 0.50:
â”‚  â”‚  â”‚  â””â”€ Likely false positive, downgrade alert
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ If not re-detected:
â”‚  â”‚     â””â”€ Keep original confidence, continue tracking
â”‚  â”‚
â”‚  â””â”€ Benefit: Resolves ambiguous detections with higher resolution
â”‚
â””â”€ Gimbal Motion Compensation (for ByteTrack):
   â”œâ”€ Read gimbal telemetry from Hub (50Hz):
   â”‚  â”œâ”€ GIMBAL_DEVICE_ATTITUDE (MAVLink message)
   â”‚  â”‚  â”œâ”€ pitch, yaw, roll (current angles in degrees)
   â”‚  â”‚  â””â”€ pitch_rate, yaw_rate, roll_rate (angular velocity)
   â”‚  â”‚
   â”‚  â””â”€ Hub provides synchronized telemetry with frames
   â”‚
   â”œâ”€ Compute image shift due to gimbal movement:
   â”‚  â”œâ”€ dt = 33ms (frame interval)
   â”‚  â”œâ”€ dx_pixels = yaw_rate Ã— focal_length Ã— dt / 57.3
   â”‚  â”œâ”€ dy_pixels = pitch_rate Ã— focal_length Ã— dt / 57.3
   â”‚  â””â”€ shift_vector = (dx_pixels, dy_pixels)
   â”‚
   â”œâ”€ Apply compensation in ByteTrack:
   â”‚  â”œâ”€ For each predicted track bbox:
   â”‚  â”‚  predicted_bbox.x += dx_pixels
   â”‚  â”‚  predicted_bbox.y += dy_pixels
   â”‚  â”œâ”€ Result: Track positions corrected for gimbal motion
   â”‚  â””â”€ Kalman filter learns TRUE object motion only
   â”‚
   â””â”€ Benefit: Prevents false ID switches when gimbal pans
    â†“
Gimbal control latency: 3ms (command generation, not including execution)
    â†“
================================================================================
                GPS COORDINATE TRANSFORMATION
================================================================================
    â†“
â”œâ”€ Inputs (all synchronized via Gimbal Hub):
â”‚  â”œâ”€ Detection bbox center: (u, v) pixels in 640Ã—640 frame
â”‚  â”œâ”€ Gimbal angles: pitch, yaw, roll (from Hub telemetry)
â”‚  â”œâ”€ Gimbal offset: (dx=0, dy=0, dz=-0.1m) from drone center
â”‚  â”œâ”€ Laser distance: D meters (from Gremsy laser via Hub, Â±2.5m accuracy)
â”‚  â”œâ”€ Drone GPS: (lat_drone, lon_drone, alt_drone) from MAVLink
â”‚  â”œâ”€ Drone attitude: roll, pitch, yaw (from IMU via MAVLink)
â”‚  â””â”€ Timestamp: Microsecond-level sync across all sensors
â”‚
â”œâ”€ Step 1: Pixel â†’ Camera Frame
â”‚  â”œâ”€ Camera intrinsics (calibration):
â”‚  â”‚  â”œâ”€ fx, fy: 1000 pixels (focal length)
â”‚  â”‚  â”œâ”€ cx, cy: 320, 256 (principal point, image center)
â”‚  â”‚  â””â”€ Distortion: k1, k2, p1, p2 (radial/tangential)
â”‚  â”‚
â”‚  â”œâ”€ Undistort pixel if needed:
â”‚  â”‚  (u', v') = undistort(u, v, distortion_coeffs)
â”‚  â”‚
â”‚  â”œâ”€ Normalized camera coordinates:
â”‚  â”‚  xc = (u' - cx) / fx
â”‚  â”‚  yc = (v' - cy) / fy
â”‚  â”‚  zc = 1.0 (unit depth)
â”‚  â”‚
â”‚  â””â”€ Ray direction in camera frame:
â”‚     ray_cam = normalize([xc, yc, zc])
â”‚
â”œâ”€ Step 2: Camera Frame â†’ Gimbal Frame
â”‚  â”œâ”€ Gimbal angles from Hub telemetry:
â”‚  â”‚  â”œâ”€ pitch_g: -90Â° (nadir) to +30Â° (forward)
â”‚  â”‚  â”œâ”€ yaw_g: -180Â° to +180Â° (azimuth)
â”‚  â”‚  â””â”€ roll_g: ~0Â° (stabilized by gimbal)
â”‚  â”‚
â”‚  â”œâ”€ Rotation matrix R_gimbal:
â”‚  â”‚  R_gimbal = Rz(yaw_g) Ã— Ry(pitch_g) Ã— Rx(roll_g)
â”‚  â”‚
â”‚  â””â”€ Ray in gimbal frame:
â”‚     ray_gimbal = R_gimbal Ã— ray_cam
â”‚
â”œâ”€ Step 3: Gimbal Frame â†’ Drone Body Frame
â”‚  â”œâ”€ Gimbal mount offset (fixed installation):
â”‚  â”‚  â”œâ”€ dx: 0m (centered)
â”‚  â”‚  â”œâ”€ dy: 0m (centered)
â”‚  â”‚  â””â”€ dz: -0.1m (below drone body)
â”‚  â”‚
â”‚  â”œâ”€ Ray in drone body frame:
â”‚  â”‚  ray_body = ray_gimbal (no rotation, gimbal aligned with body)
â”‚  â”‚
â”‚  â””â”€ Apply gimbal offset:
â”‚     ray_body_origin = drone_center + offset
â”‚
â”œâ”€ Step 4: Drone Body Frame â†’ World Frame (NED)
â”‚  â”œâ”€ Drone attitude from IMU:
â”‚  â”‚  â”œâ”€ roll_d: Typically Â±5Â° during flight
â”‚  â”‚  â”œâ”€ pitch_d: Typically Â±10Â° during flight
â”‚  â”‚  â””â”€ yaw_d: 0-360Â° (heading)
â”‚  â”‚
â”‚  â”œâ”€ Rotation matrix R_drone:
â”‚  â”‚  R_drone = Rz(yaw_d) Ã— Ry(pitch_d) Ã— Rx(roll_d)
â”‚  â”‚
â”‚  â”œâ”€ Ray in NED (North-East-Down) frame:
â”‚  â”‚  ray_NED = R_drone Ã— ray_body
â”‚  â”‚
â”‚  â”œâ”€ Scale by laser distance:
â”‚  â”‚  point_NED = ray_NED Ã— laser_distance_D
â”‚  â”‚
â”‚  â””â”€ Result: 3D offset from drone in NED coordinates
â”‚     (north_offset, east_offset, down_offset) in meters
â”‚
â”œâ”€ Step 5: NED â†’ WGS84 GPS Coordinates
â”‚  â”œâ”€ Drone GPS (reference point):
â”‚  â”‚  lat_drone, lon_drone, alt_drone (WGS84)
â”‚  â”‚
â”‚  â”œâ”€ Convert NED to lat/lon change:
â”‚  â”‚  â”œâ”€ Earth radius: R = 6,378,137m (WGS84 equatorial)
â”‚  â”‚  â”œâ”€ lat_change = north_offset / R Ã— (180/Ï€)
â”‚  â”‚  â”œâ”€ lon_change = east_offset / (R Ã— cos(lat_drone)) Ã— (180/Ï€)
â”‚  â”‚  â””â”€ alt_change = -down_offset (NED down is negative altitude)
â”‚  â”‚
â”‚  â””â”€ Target GPS coordinates:
â”‚     lat_target = lat_drone + lat_change
â”‚     lon_target = lon_drone + lon_change
â”‚     alt_target = alt_drone + alt_change
â”‚
â”œâ”€ Accuracy Analysis:
â”‚  â”œâ”€ Laser distance: Â±2.5m (Gremsy spec)
â”‚  â”œâ”€ Gimbal angles: Â±0.1Â° (encoder precision)
â”‚  â”œâ”€ Drone GPS: Â±2m (RTK) or Â±5m (standard)
â”‚  â”œâ”€ Total horizontal: Â±2.5m with RTK, Â±5m without
â”‚  â””â”€ Vertical: Â±1m (barometric + laser)
â”‚
â””â”€ Output: Single GPS point per track per frame
   {lat: float, lon: float, alt: float, accuracy_m: float, timestamp_ms: int}
    â†“
GPS transformation latency: 2ms
    â†“
================================================================================
                ALERT GENERATION & OPERATOR DASHBOARD
================================================================================
    â†“
â”œâ”€ Alert Classification (Person + Weapon combined):
â”‚  â”‚
â”‚  â”œâ”€ VERY HIGH PRIORITY (ðŸ”´ Immediate dispatch):
â”‚  â”‚  â”œâ”€ metal_rod â‰¥ 0.82 confidence
â”‚  â”‚  â”œâ”€ OR: Zoom re-detection confirmed (+20% boost applied)
â”‚  â”‚  â”œâ”€ OR: knife_machete â‰¥ 0.78 with thermal confirmation
â”‚  â”‚  â”œâ”€ Operator action: Dispatch law enforcement immediately
â”‚  â”‚  â””â”€ Review time: 1-2 seconds
â”‚  â”‚
â”‚  â”œâ”€ HIGH PRIORITY (ðŸŸ  Monitor closely):
â”‚  â”‚  â”œâ”€ knife_machete â‰¥ 0.72
â”‚  â”‚  â”œâ”€ OR: metal_rod â‰¥ 0.72
â”‚  â”‚  â”œâ”€ OR: Person-weapon association confirmed
â”‚  â”‚  â”œâ”€ Operator action: Monitor + prepare dispatch
â”‚  â”‚  â””â”€ Review time: 2-3 seconds
â”‚  â”‚
â”‚  â”œâ”€ MEDIUM PRIORITY (ðŸŸ¡ Investigate):
â”‚  â”‚  â”œâ”€ knife_machete â‰¥ 0.65
â”‚  â”‚  â”œâ”€ OR: Track persists â‰¥ 5 frames
â”‚  â”‚  â”œâ”€ OR: Thermal boost applied
â”‚  â”‚  â”œâ”€ Operator action: Request gimbal zoom, investigate
â”‚  â”‚  â””â”€ Review time: 3-5 seconds
â”‚  â”‚
â”‚  â””â”€ LOW PRIORITY (ðŸŸ¢ Verify):
â”‚     â”œâ”€ Any weapon â‰¥ 0.55
â”‚     â”œâ”€ Likely false positive (pipe on bike, mirror reflection)
â”‚     â”œâ”€ Operator action: Quick verify, usually dismiss
â”‚     â””â”€ Review time: 4-5 seconds
â”‚
â”œâ”€ Alert Package Contents:
â”‚  â”œâ”€ Video evidence:
â”‚  â”‚  â”œâ”€ Current frame (640Ã—640, annotated with bboxes)
â”‚  â”‚  â”œâ”€ Â±5 frame context (10 frames total, ~330ms)
â”‚  â”‚  â”œâ”€ Zoomed frame if available
â”‚  â”‚  â””â”€ Thermal overlay side-by-side
â”‚  â”‚
â”‚  â”œâ”€ Detection metadata:
â”‚  â”‚  â”œâ”€ Weapon class: "Machete" / "Crowbar" / "Metal Pipe"
â”‚  â”‚  â”œâ”€ Confidence: 0-100% (visual bar + number)
â”‚  â”‚  â”œâ”€ Thermal confidence: 0-100% (separate indicator)
â”‚  â”‚  â”œâ”€ Combined confidence: Weighted fusion result
â”‚  â”‚  â””â”€ Boost reason: "Thermal verified" / "Zoom confirmed"
â”‚  â”‚
â”‚  â”œâ”€ Person association:
â”‚  â”‚  â”œâ”€ Owner track ID: "Person #42"
â”‚  â”‚  â”œâ”€ Person description: "Rider on motorcycle"
â”‚  â”‚  â”œâ”€ Clothing color (if detectable): "Red shirt"
â”‚  â”‚  â””â”€ Motorcycle type (if detected): "Honda Wave"
â”‚  â”‚
â”‚  â”œâ”€ Location data:
â”‚  â”‚  â”œâ”€ GPS coordinates: 10.7769Â° N, 106.6970Â° E
â”‚  â”‚  â”œâ”€ Accuracy: Â±2.5m
â”‚  â”‚  â”œâ”€ Altitude: 52m
â”‚  â”‚  â”œâ”€ Street name (geocoded): "Nguyen Hue St, District 1"
â”‚  â”‚  â””â”€ Map thumbnail: Mini-map with location marker
â”‚  â”‚
â”‚  â”œâ”€ Tracking data:
â”‚  â”‚  â”œâ”€ Track ID: weapon_20260109_143052_001
â”‚  â”‚  â”œâ”€ Track age: 47 frames (1.6 seconds)
â”‚  â”‚  â”œâ”€ Trajectory: GPS polyline on map
â”‚  â”‚  â”œâ”€ Direction: Heading 45Â° NE
â”‚  â”‚  â””â”€ Speed estimate: ~30 km/h (motorcycle typical)
â”‚  â”‚
â”‚  â”œâ”€ Timestamp:
â”‚  â”‚  â”œâ”€ Frame number: 142,857
â”‚  â”‚  â”œâ”€ Unix timestamp: 1736416252.347
â”‚  â”‚  â””â”€ Human readable: "2026-01-09 14:30:52"
â”‚  â”‚
â”‚  â””â”€ Operator actions:
â”‚     â”œâ”€ [CONFIRM] - Dispatch police, save evidence
â”‚     â”œâ”€ [DISMISS] - False positive, discard
â”‚     â”œâ”€ [INVESTIGATE] - Request gimbal zoom
â”‚     â”œâ”€ [FLAG] - Suspicious but uncertain
â”‚     â””â”€ [NOTE] - Add free-text observation
â”‚
â””â”€ Dashboard User Interface:
   â”œâ”€ Main video feed: 640Ã—640 or 1920Ã—1080 (selectable)
   â”‚  â”œâ”€ Bounding box overlays (color-coded by class)
   â”‚  â”œâ”€ Track ID labels (e.g., "K#42" for knife track 42)
   â”‚  â”œâ”€ Confidence percentages
   â”‚  â””â”€ Person-weapon links (dashed lines)
   â”‚
   â”œâ”€ Detection history panel (right side):
   â”‚  â”œâ”€ Last 50 detections (scrollable)
   â”‚  â”œâ”€ Filter by: class, confidence, alert level
   â”‚  â””â”€ Click to jump to frame
   â”‚
   â”œâ”€ Alert queue (top):
   â”‚  â”œâ”€ New alerts (highest priority first)
   â”‚  â”œâ”€ Audio: Beep for HIGH, Alarm for VERY_HIGH
   â”‚  â””â”€ Auto-dismiss after 30s if LOW priority
   â”‚
   â”œâ”€ Map view (bottom-left):
   â”‚  â”œâ”€ City map (OpenStreetMap / Google Maps)
   â”‚  â”œâ”€ Drone position marker
   â”‚  â”œâ”€ Weapon GPS locations (color-coded icons)
   â”‚  â””â”€ Track trajectories (polylines)
   â”‚
   â”œâ”€ Gimbal control panel (bottom-right):
   â”‚  â”œâ”€ Current pitch/yaw angles
   â”‚  â”œâ”€ Zoom level indicator
   â”‚  â”œâ”€ Manual override buttons
   â”‚  â””â”€ "Center on Track #X" quick button
   â”‚
   â””â”€ Statistics panel:
      â”œâ”€ Alerts today: 12 (3 HIGH, 5 MEDIUM, 4 LOW)
      â”œâ”€ Confirmed weapons: 2
      â”œâ”€ False positive rate: 18%
      â””â”€ System status: âœ… All sensors OK
    â†“
Alert generation latency: <1ms
    â†“
================================================================================
                EVIDENCE LOGGING & CLOUD SYNC
================================================================================
    â†“
â”œâ”€ Local Storage (Drone SD Card):
â”‚  â”‚
â”‚  â”œâ”€ Continuous recording:
â”‚  â”‚  â”œâ”€ Full H.264 video: All 30 FPS, quality preset "High"
â”‚  â”‚  â”œâ”€ Bitrate: ~15 Mbps (1.8 GB/hour)
â”‚  â”‚  â”œâ”€ Filename: flight_20260109_143000.mp4
â”‚  â”‚  â””â”€ Duration: Continuous during flight
â”‚  â”‚
â”‚  â”œâ”€ Metadata stream (JSONL format, 1KB per frame):
â”‚  â”‚  {
â”‚  â”‚    "frame_id": 142857,
â”‚  â”‚    "timestamp_ms": 1736416252347,
â”‚  â”‚    "detections": [
â”‚  â”‚      {"bbox": [0.42, 0.38, 0.08, 0.15], "class": 1, "conf": 0.78,
â”‚  â”‚       "thermal_conf": 0.65, "owner_id": 42}
â”‚  â”‚    ],
â”‚  â”‚    "tracks": [
â”‚  â”‚      {"track_id": 87, "class": 1, "state": "ACTIVE", "age": 47}
â”‚  â”‚    ],
â”‚  â”‚    "gps": {"lat": 10.7769, "lon": 106.6970, "alt": 52.0},
â”‚  â”‚    "gimbal": {"pitch": -90.0, "yaw": 15.2, "zoom": 4.0},
â”‚  â”‚    "laser_distance": 51.3,
â”‚  â”‚    "thermal_features": {"F1": 0.72, "F2": 0.45, "F3": 0.88, "F4": 0.67, "F5": 0.91}
â”‚  â”‚  }
â”‚  â”‚
â”‚  â”œâ”€ Evidence package (created on CONFIRM):
â”‚  â”‚  â”œâ”€ Folder: /evidence/20260109/weapon_001/
â”‚  â”‚  â”œâ”€ Contents:
â”‚  â”‚  â”‚  â”œâ”€ clip.mp4: 10-second video (Â±5 sec from trigger)
â”‚  â”‚  â”‚  â”œâ”€ frame_001.jpg ... frame_005.jpg: 5 key frames (1080p)
â”‚  â”‚  â”‚  â”œâ”€ thermal_001.png ... thermal_005.png: Thermal frames
â”‚  â”‚  â”‚  â”œâ”€ trajectory.geojson: GPS track as GeoJSON polyline
â”‚  â”‚  â”‚  â”œâ”€ metadata.json: Complete sensor readings
â”‚  â”‚  â”‚  â”œâ”€ operator_notes.txt: Free-text observations
â”‚  â”‚  â”‚  â””â”€ signature.sha256: Digital checksum
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Chain of custody:
â”‚  â”‚     â”œâ”€ created_by: "operator_badge_12345"
â”‚  â”‚     â”œâ”€ created_at: "2026-01-09T14:31:05Z"
â”‚  â”‚     â””â”€ hash: SHA-256 of all files
â”‚  â”‚
â”‚  â””â”€ Retention policy:
â”‚     â”œâ”€ Evidence packages: Keep indefinitely (until case closed)
â”‚     â”œâ”€ Full video: 30 days rolling
â”‚     â””â”€ Metadata: 90 days rolling
â”‚
â””â”€ Cloud Sync (LTE Modem, Asynchronous):
   â”‚
   â”œâ”€ Trigger conditions:
   â”‚  â”œâ”€ Immediate: Evidence confirmed by operator
   â”‚  â”œâ”€ Scheduled: Daily at 11 PM (bulk upload)
   â”‚  â””â”€ Manual: Operator requests sync
   â”‚
   â”œâ”€ Upload pipeline:
   â”‚  â”œâ”€ Compress: H.264 already compressed, skip
   â”‚  â”œâ”€ Encrypt: AES-256-GCM with per-file key
   â”‚  â”œâ”€ Chunk: Split into 10MB parts (LTE reliability)
   â”‚  â”œâ”€ Upload: HTTPS PUT to AWS S3
   â”‚  â”œâ”€ Retry: 3Ã— with exponential backoff (1s, 5s, 30s)
   â”‚  â””â”€ Verify: SHA-256 checksum after upload
   â”‚
   â”œâ”€ Cloud storage structure:
   â”‚  â””â”€ s3://weapon-evidence-bucket/
   â”‚     â””â”€ evidence/
   â”‚        â””â”€ 2026/
   â”‚           â””â”€ 01/
   â”‚              â””â”€ 09/
   â”‚                 â””â”€ weapon_001_1736416252/
   â”‚                    â”œâ”€ clip.mp4.enc (encrypted)
   â”‚                    â”œâ”€ metadata.json.enc
   â”‚                    â””â”€ manifest.json
   â”‚
   â”œâ”€ Backend processing:
   â”‚  â”œâ”€ RDS database: Index metadata for search
   â”‚  â”œâ”€ Elasticsearch: Full-text search on notes
   â”‚  â”œâ”€ Lambda: Generate thumbnails, transcode video
   â”‚  â””â”€ API Gateway: REST interface for law enforcement
   â”‚
   â””â”€ Legal compliance:
      â”œâ”€ Encryption: AES-256 at rest and in transit
      â”œâ”€ Access control: Role-based (admin, operator, law enforcement)
      â”œâ”€ Audit log: Every access logged with IP, timestamp, user
      â”œâ”€ Retention: Auto-delete after 90 days unless flagged
      â””â”€ Export: Generate court-admissible evidence package


================================================================================
                        LATENCY SUMMARY
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE                       â”‚ LATENCY   â”‚ CUMULATIVE â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hub â†’ Jetson transfer       â”‚ 2ms       â”‚ 2ms        â”‚
â”‚ Preprocessing               â”‚ 3ms       â”‚ 5ms        â”‚
â”‚ RGB backbone                â”‚ 7ms       â”‚ 12ms       â”‚
â”‚ Thermal branch              â”‚ 2ms       â”‚ 14ms       â”‚
â”‚ CBAM fusion                 â”‚ 1ms       â”‚ 15ms       â”‚
â”‚ Detection head              â”‚ 2ms       â”‚ 17ms       â”‚
â”‚ NMS + filtering             â”‚ 1ms       â”‚ 18ms       â”‚
â”‚ Person-weapon linking       â”‚ 1ms       â”‚ 19ms       â”‚
â”‚ Thermal features (5)        â”‚ 1ms       â”‚ 20ms       â”‚
â”‚ LSTM verification           â”‚ 1ms       â”‚ 21ms       â”‚
â”‚ ByteTrack                   â”‚ 2ms       â”‚ 23ms       â”‚
â”‚ Gimbal command              â”‚ 2ms       â”‚ 25ms       â”‚
â”‚ GPS transformation          â”‚ 2ms       â”‚ 27ms       â”‚
â”‚ Alert generation            â”‚ 1ms       â”‚ 28ms       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL PIPELINE              â”‚ 28ms      â”‚ âœ… < 33ms  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Budget: 33ms for 30 FPS â†’ Headroom: 5ms (15% margin)


================================================================================
                        MODULE FILE MAPPING
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMPONENT               â”‚ SOURCE FILE                    â”‚ KEY CLASS                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gimbal Hub              â”‚ src/gimbal/hub.py              â”‚ GimbalHub, FramePacket   â”‚
â”‚ Detection + CBAM        â”‚ src/detection/detector.py      â”‚ WeaponDetector, CBAMFusionâ”‚
â”‚ Person-Weapon Linking   â”‚ src/detection/linker.py        â”‚ PersonWeaponLinker       â”‚
â”‚ Thermal Features        â”‚ src/thermal/verifier.py        â”‚ ThermalFeatureExtractor  â”‚
â”‚ Thermal LSTM            â”‚ src/thermal/verifier.py        â”‚ ThermalLSTM, ThermalVerifierâ”‚
â”‚ ByteTrack               â”‚ src/tracking/tracker.py        â”‚ ByteTracker, Track       â”‚
â”‚ Gimbal Controller       â”‚ src/gimbal/controller.py       â”‚ GimbalController         â”‚
â”‚ GPS Transformer         â”‚ src/gps/transformer.py         â”‚ GPSTransformer           â”‚
â”‚ Alert System            â”‚ src/evidence/alerts.py         â”‚ AlertGenerator           â”‚
â”‚ Evidence Collector      â”‚ src/evidence/collector.py      â”‚ EvidenceCollector        â”‚
â”‚ Main Pipeline           â”‚ src/pipeline.py                â”‚ WeaponDetectionPipeline  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Document Version**: v7.0  
**Last Updated**: January 9, 2026  
**Status**: âœ… Complete with Person Detection + Gimbal Hub Integration
