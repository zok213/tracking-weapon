# VT-MOT Tracking Weapon ðŸŽ¯

RGBT Multi-Object Tracking with Gated Mid-Fusion for Deployment.

## Quick Start (After Clone)

```bash
# 1. Install dependencies
pip install -e ./YOLOv11-RGBT

# 2. Download dataset from Kaggle
# Upload vtmot_far_*.zip parts, then extract:
# unzip vtmot_far_train_part1.zip -d datasets/vtmot_far/
# unzip vtmot_far_train_part2.zip -d datasets/vtmot_far/
# ... etc

# 3. Download weights (from original machine backup)
# Place in weights/ directory

# 4. Train far-view model
python3 train_far_model_gated.py
```

## Project Structure

```
â”œâ”€â”€ train_far_model_gated.py    # ðŸŽ¯ MAIN: Far-view deployment training
â”œâ”€â”€ train_near_model_gated.py   # Near-view gated fusion experiment
â”œâ”€â”€ gate_supervision.py         # Gate supervision loss module
â”œâ”€â”€ visualize_gates.py          # Gate weight visualization
â”œâ”€â”€ mcf_utils.py                # MCF utility functions
â”œâ”€â”€ YOLOv11-RGBT/               # Modified Ultralytics with GatedSpatialFusion_V3
â”‚   â””â”€â”€ ultralytics/
â”‚       â””â”€â”€ nn/modules/block.py # â­ GatedSpatialFusion_V3 implementation
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ vtmot_far/far_view_clean.yaml  # Far-view dataset config
â”‚   â””â”€â”€ vtmot_near/near_view_clean.yaml
â”œâ”€â”€ weights/                    # Pretrained weights (not in git, >100MB)
â”œâ”€â”€ docs/                       # Architecture docs, analysis, walkthroughs
â””â”€â”€ scripts/                    # Utility scripts
```

## Key Architecture

**Gated Spatial Fusion V3** â€” Custom RGBT fusion layer:

- Dual-branch attention (RGB + Thermal)
- MC-Dropout uncertainty estimation
- Learnable illumination scaling
- Gate supervision loss for convergence

## Training Strategy

1. **Near-view warmup** â†’ best.pt (mAP50=0.635)
2. **Far-view fine-tune** â†’ Transfer Learning V2 (nearâ†’far domain adaptation)

## Dataset

- **vtmot_far**: 284k images (Train 74%, Val 16%, Test 11%)
- Sources: VTuav, wurenji, qiuxing, RGBT234, photo sequences
- Single class: person (far-view detection)
